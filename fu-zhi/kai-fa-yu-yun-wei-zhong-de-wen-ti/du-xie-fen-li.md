# 读写分离

对于读占比较高的场景，可以通过把一部分读流量分摊到从节点 （slave）来减轻主节点（master）压力，同时需要注意永远只对主节点执行写操作，如图所示。

![](../../.gitbook/assets/image%20%2845%29.png)

当使用从节点响应读请求时，业务端可能会遇到如下问题： 

* 复制数据延迟。 
* 读到过期数据。 
* 从节点故障。

## 数据延迟

Redis复制数据的延迟由于异步复制特性是无法避免的，延迟取决于网络带宽和命令阻塞情况，比如刚在主节点写入数据后立刻在从节点上读取可能获取不到。需要业务场景允许短时间内的数据延迟。对于无法容忍大量延 迟场景，可以编写外部监控程序监听主从节点的复制偏移量，当延迟较大时触发报警或者通知客户端避免读取延迟过高的从节点，实现逻辑如图所示。

![](../../.gitbook/assets/image%20%2892%29.png)

流程如下：

### 监控程序检查

监控程序（monitor）定期检查主从节点的偏移量，主节点偏移量在info replication的master\_repl\_offset指标记录，从节点偏移量可以查询主节点的slave0字段的offset指标，它们的差值就是主从节点延迟的字节量。

### 延迟警告

当延迟字节量过高时，比如超过10MB。监控程序触发报警并通知客户端从节点延迟过高。可以采用Zookeeper的监听回调机制实现客户端通知。

### 修改读取位置

客户端接到具体的从节点高延迟通知后，修改读命令路由到其他从节点或主节点上。当延迟恢复后，再次通知客户端，恢复从节点的读命令请求。

这种方案的成本比较高，需要单独修改适配Redis的客户端类库。如果 涉及多种语言成本将会扩大。客户端逻辑需要识别出读写请求并自动路由， 还需要维护故障和恢复的通知。采用此方案视具体的业务而定，如果允许不 一致性或对延迟不敏感的业务可以忽略，也可以采用Redis集群方案做水平扩展。

## 读到过期数据

当主节点存储大量设置超时的数据时，如缓存数据，Redis内部需要维 护过期数据删除策略，删除策略主要有两种：惰性删除和定时删除，具体细节见[内存管理](../../nei-cun/li-jie-nei-cun.md)。

### 惰性删除

主节点每次处理读取命令时，都会检查键是否超时，如果超时则执行del命令删除键对象，之后del命令也会异步发送给从节点。需要注意的是为了保证复制的一致性，从节点自身永远不会主动删除超时数据，如图所示。

![](../../.gitbook/assets/image%20%2827%29.png)

### 定时删除

Redis主节点在内部定时任务会循环采样一定数量的键，当发现采样的键过期时执行del命令，之后再同步给从节点，如图所示。

![](../../.gitbook/assets/image%20%287%29.png)

如果此时数据大量超时，主节点采样速度跟不上过期速度且主节点没有读取过期键的操作，那么从节点将无法收到del命令。这时在从节点上可以读取到已经超时的数据。Redis在3.2版本解决了这个问题，从节点读取数据之前会检查键的过期时间来决定是否返回数据，可以升级到3.2版本来规避这个问题。

##  从结点故障问题

对于从节点的故障问题，需要在客户端维护可用从节点列表，当从节点故障时立刻切换到其他从节点或主节点上。这个过程类似上文提到的针对延迟过高的监控处理，需要开发人员改造客户端类库。

## 总结

综上所述，使用Redis做读写分离存在一定的成本。Redis本身的性能非常高，开发人员在使用额外的从节点提升读性能之前，尽量在主节点上做充分优化，比如解决慢查询，持久化阻塞，合理应用数据结构等，当主节点优化空间不大时再考虑扩展。笔者建议大家在做读写分离之前，可以考虑**使用Redis Cluster等分布式解决方案，这样不止扩展了读性能还可以扩展写性能和可支撑数据规模，并且一致性和故障转移也可以得到保证，对于客户端的维护逻辑也相对容易。**

